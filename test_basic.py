"""
Script de teste b√°sico para verificar se o scraper est√° funcionando
Testa apenas a coleta de not√≠cias sem precisar de Supabase, WordPress ou Gemini
"""
import sys
import os

# Adiciona shared ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'shared'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'services/blabbermouth'))

import requests
from bs4 import BeautifulSoup
from datetime import datetime

def test_feed_access():
    """Testa se consegue acessar o feed RSS do Blabbermouth"""
    print("üîç Testando acesso ao feed RSS do Blabbermouth...")
    
    feed_url = "https://www.blabbermouth.net/feed/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(feed_url, headers=headers, timeout=10)
        response.raise_for_status()
        print(f"‚úÖ Feed acessado com sucesso! Status: {response.status_code}")
        return response.content
    except Exception as e:
        print(f"‚ùå Erro ao acessar feed: {e}")
        return None

def test_feed_parsing(feed_content):
    """Testa se consegue fazer parse do feed RSS"""
    print("\nüì∞ Testando parsing do feed RSS...")
    
    try:
        soup = BeautifulSoup(feed_content, "xml")
        articles = soup.find_all("item")
        print(f"‚úÖ Feed parseado com sucesso! Encontrados {len(articles)} artigos")
        
        if articles:
            # Mostra informa√ß√µes do primeiro artigo
            first_article = articles[0]
            title = first_article.find("title")
            link = first_article.find("link")
            pub_date = first_article.find("pubDate")
            
            print(f"\nüìÑ Primeiro artigo encontrado:")
            if title:
                print(f"   T√≠tulo: {title.text.strip()[:80]}...")
            if link:
                print(f"   Link: {link.text.strip()}")
            if pub_date:
                print(f"   Data: {pub_date.text.strip()}")
        
        return articles[:3]  # Retorna os 3 primeiros para teste
    except Exception as e:
        print(f"‚ùå Erro ao fazer parse do feed: {e}")
        return []

def test_article_details(articles):
    """Testa se consegue acessar detalhes de um artigo"""
    print("\nüîé Testando acesso aos detalhes de um artigo...")
    
    if not articles:
        print("‚ö†Ô∏è Nenhum artigo para testar")
        return
    
    first_article = articles[0]
    link = first_article.find("link")
    
    if not link:
        print("‚ö†Ô∏è Artigo sem link")
        return
    
    article_url = link.text.strip()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        print(f"   Acessando: {article_url}")
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()
        print(f"‚úÖ Artigo acessado com sucesso! Status: {response.status_code}")
        
        # Tenta extrair conte√∫do
        soup = BeautifulSoup(response.content, "html.parser")
        
        # Tenta encontrar conte√∫do
        content_selectors = [
            "div.news-content",
            "div.article-content",
            "div.post-content",
            "div.entry-content",
            "article",
            "div.content"
        ]
        
        content_found = False
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text(separator="\n").strip()
                if content:
                    print(f"‚úÖ Conte√∫do extra√≠do! Tamanho: {len(content)} caracteres")
                    print(f"   Preview: {content[:100]}...")
                    content_found = True
                    break
        
        if not content_found:
            print("‚ö†Ô∏è Conte√∫do n√£o encontrado com os seletores padr√£o")
        
        # Tenta encontrar imagem
        og_image = soup.find("meta", property="og:image")
        if og_image and og_image.get("content"):
            print(f"‚úÖ Imagem encontrada: {og_image['content'][:80]}...")
        else:
            print("‚ö†Ô∏è Imagem n√£o encontrada")
        
        return True
    except Exception as e:
        print(f"‚ùå Erro ao acessar artigo: {e}")
        return False

def main():
    """Executa todos os testes"""
    print("=" * 60)
    print("üß™ TESTE B√ÅSICO DO SCRAPER BLABBERMOUTH")
    print("=" * 60)
    
    # Teste 1: Acesso ao feed
    feed_content = test_feed_access()
    if not feed_content:
        print("\n‚ùå Teste falhou: n√£o foi poss√≠vel acessar o feed")
        return
    
    # Teste 2: Parsing do feed
    articles = test_feed_parsing(feed_content)
    if not articles:
        print("\n‚ùå Teste falhou: n√£o foi poss√≠vel fazer parse do feed")
        return
    
    # Teste 3: Detalhes do artigo
    test_article_details(articles)
    
    print("\n" + "=" * 60)
    print("‚úÖ TESTE B√ÅSICO CONCLU√çDO!")
    print("=" * 60)
    print("\nüí° O scraper est√° funcionando corretamente para coleta de not√≠cias.")
    print("   Para usar todas as funcionalidades, configure:")
    print("   - SUPABASE_URL e SUPABASE_KEY (para armazenamento)")
    print("   - GEMINI_API_KEY (para tradu√ß√£o)")
    print("   - WORDPRESS_URL, WORDPRESS_USER, WORDPRESS_APP_PASSWORD (para publica√ß√£o)")

if __name__ == "__main__":
    main()

